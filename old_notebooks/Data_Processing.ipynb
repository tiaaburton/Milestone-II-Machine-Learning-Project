{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data Processing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1xhP-JY0jMqxzhuXr-1AktsVsJ8ZcEzVC",
      "authorship_tag": "ABX9TyOqco0kIh0Toe2Gb5/V5gFF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tiaaburton/Milestone-II-Machine-Learning-Project/blob/main/Data_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dv21GYSVlIFa",
        "outputId": "c6837815-37fc-4903-dfe2-4c375e61adb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.4 MB 34 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.3\n",
            "  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 59.1 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=a9755b16ed33a40363fdcd7b8ac5f3c6db661dfedcee415b653ae995542b70ef\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/f5/07/7cd8017084dce4e93e84e92efd1e1d5334db05f2e83bcef74f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkafcB1sjvNT",
        "outputId": "ab84e11b-75c2-477b-abac-f310fab9cd60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "import pyspark\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re"
      ],
      "metadata": {
        "id": "Dn-tcYpjshiV"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"Colab 6M Exploration\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()"
      ],
      "metadata": {
        "id": "B3ZUzvNwsFUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.newSession()"
      ],
      "metadata": {
        "id": "BN8wqFRocQ5N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "5da6741b-41dc-4f53-8661-094988d77db1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f98f9f58750>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://1c7289105be7:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.2.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Colab 6M Exploration</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "LfNWMW17Y4Rs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "schema = StructType([\\\n",
        "    StructField(\"visitorId\", StringType(), True),\\\n",
        "    StructField(\"visitNumber\", StringType(), True),\\\n",
        "    StructField(\"visitId\", StringType(), True),\\\n",
        "    StructField(\"visitStartTime\", StringType(), True),\\\n",
        "    StructField(\"date\", StringType(), True),\\\n",
        "    StructField(\"totals\", StringType(), True),\\\n",
        "    StructField(\"trafficSource\", StringType(), True),\\\n",
        "    StructField(\"device\", StringType(), True),\\\n",
        "    StructField(\"geoNetwork\", StringType(), True),\\\n",
        "    StructField(\"customDimensions\", StringType(), True),\\\n",
        "    StructField(\"hits\", StringType(), True),\\\n",
        "    StructField(\"userId\", StringType(), True),\\\n",
        "    StructField(\"clientId\", StringType(), True),\\\n",
        "    StructField(\"channelGrouping\", StringType(), True),\\\n",
        "    StructField(\"socialEngagementType\", StringType(), True)])"
      ],
      "metadata": {
        "id": "gFq6IL3V6hSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_selected_columns(filename='/content/drive/Shareddrives/SIADS - 694-695 Team Drive/Notebooks/final_columns.txt'):\n",
        "  \"\"\"\n",
        "  filename: \"\"\"\n",
        "  columns = []\n",
        "  with open(filename, 'r') as f:\n",
        "    for ln in f.readlines():\n",
        "      col = (ln.strip().split('\\xa0\\xa0')[1])\n",
        "      groups = re.search(r\"'(\\w.+)'\", col)\n",
        "      columns.append(groups.group(1))\n",
        "  return columns\n",
        "\n",
        "def read_csv(filename):\n",
        "  \"\"\"\n",
        "  Reads csv in with spark dataframe. \n",
        "  \n",
        "  Params \n",
        "    filename\n",
        "  \n",
        "  Output\n",
        "    Pyspark.sql dataframe object with selected columns in\"\"\"\n",
        "  columns = get_selected_columns()\n",
        "\n",
        "  selection = []\n",
        "  for col in columns:\n",
        "    splits = col.split('.')\n",
        "    if splits[0] not in selection + ['clientId']:\n",
        "      selection.append(splits[0])\n",
        "\n",
        "  data = spark.read\\\n",
        "  .option('header','true')\\\n",
        "  .option('delimiter',',')\\\n",
        "  .option('index','false')\\\n",
        "  .option('mode','DROPMALFORMED')\\\n",
        "  .csv(filename)\n",
        "\n",
        "  data = data.select(selection)\n",
        "\n",
        "  return data\n",
        "\n",
        "def add_col_from_json(dataframe, json_col, json_field):\n",
        "  \"\"\"\n",
        "  json_col:\n",
        "  json_field: \"\"\"\n",
        "  fieldname = '$.'+json_field\n",
        "  df = dataframe.withColumn(json_field, get_json_object(dataframe[json_col],fieldname))\n",
        "  return df"
      ],
      "metadata": {
        "id": "zB4bZ6TExG57"
      },
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "eJPR5QSjKWkz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns = get_selected_columns('/content/drive/Shareddrives/SIADS - 694-695 Team Drive/Notebooks/final_columns.txt')\n",
        "analytics_data = read_csv('/content/drive/Shareddrives/SIADS - 694-695 Team Drive/Sample Datasets/2016 - 2017 Google Analytics Dataset.csv')\n",
        "\n",
        "drop_cols = set()\n",
        "\n",
        "for col in columns:\n",
        "  splits = col.split('.')\n",
        "  if len(splits) == 2:\n",
        "    analytics_data = add_col_from_json(analytics_data, splits[0], splits[1])\n",
        "  elif len(splits) == 3:\n",
        "    drop_cols.add(splits[0])\n",
        "    drop_cols.add(splits[1])\n",
        "    analytics_data = add_col_from_json(analytics_data, splits[0], splits[1])\n",
        "    analytics_data = add_col_from_json(analytics_data, splits[1], splits[2])"
      ],
      "metadata": {
        "id": "ZHLi9QFj3S76"
      },
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analytics_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NuSlgd3BnvJX",
        "outputId": "7eea3279-566c-43bc-fed0-0b1fe59b8978"
      },
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[totals: string, fullVisitorId: string, hits: string, channelGrouping: string, socialEngagementType: string, geoNetwork: string, trafficSource: string, device: string, visits: string, pageviews: string, timeOnSite: string, bounces: string, transactions: string, newVisits: string, screenviews: string, uniqueScreenviews: string, timeOnScreen: string, totalTransactionRevenue: string, type: string, country: string, source: string, medium: string, isTrueDirect: string, browser: string, operatingSystem: string, deviceCategory: string, hour: string, minute: string, transaction: string, item: string, dataSource: string, transactionRevenue: string, productName: string, productCategory: string, itemRevenue: string, appInfo: string, screenDepth: string, eCommerceAction: string, action_type: string]"
            ]
          },
          "metadata": {},
          "execution_count": 187
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# spark.write.csv(output_path)"
      ],
      "metadata": {
        "id": "0ckbpXLVR9dA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pyspark.sql.SparkSession.stop"
      ],
      "metadata": {
        "id": "UE93BMazlusi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}