{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"orchestrator.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Pipeline Workflow\n","\n","This noteboook includes the workflow for both processes:\n","1. Data Pipeline\n","2. Model Pipeline\n","\n","TODO:\n","- @All align on the python files/naming conventions and each steps\n","- @All make sure the steps reflect the one Fred proposed"],"metadata":{"id":"d3ZkjF-yw6We"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JZGmt1OuwdLb","executionInfo":{"status":"ok","timestamp":1652574422852,"user_tz":420,"elapsed":33353,"user":{"displayName":"Tia Burton","userId":"09352393735441630552"}},"outputId":"8cfd0560-7a43-4b92-c305-92343979e5c2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["%cd \"/content/drive/Shareddrives/SIADS - 694-695 Team Drive/python-files\"\n","!ls -l"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pYasLeTo5BIq","executionInfo":{"status":"ok","timestamp":1652545931428,"user_tz":240,"elapsed":914,"user":{"displayName":"Idris Hanafi","userId":"08993226175925256732"}},"outputId":"45258ea6-f265-4bf5-a178-36b15a9f99a7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/Shareddrives/SIADS - 694-695 Team Drive/python-files\n","total 67\n","-rw------- 1 root root  1655 May 13 02:13 clean_dataset.py\n","-rw------- 1 root root  2434 May 12 00:19 create_sample_dataset.py\n","-rw------- 1 root root  3327 May 11 22:09 data_extraction.py\n","-rw------- 1 root root 17254 May 12 20:26 data_prep.py\n","-rw------- 1 root root     0 Apr 23 23:30 data_preprocessing.py\n","-rw------- 1 root root   165 May  3 22:13 ml_algo.py\n","-rw------- 1 root root 21978 May 13 02:16 orchestrator.ipynb\n","drwx------ 2 root root  4096 May 11 04:15 __pycache__\n","-rw------- 1 root root 10931 May 12 17:51 transaction_extraction.py\n","-rw------- 1 root root  4130 May 12 00:15 utils.py\n"]}]},{"cell_type":"code","source":["!python --version"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S5zc5gw9wnxZ","executionInfo":{"status":"ok","timestamp":1652545931618,"user_tz":240,"elapsed":193,"user":{"displayName":"Idris Hanafi","userId":"08993226175925256732"}},"outputId":"fd6b1799-a643-4791-d136-4f8795093bb5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Python 3.7.13\n"]}]},{"cell_type":"markdown","source":["If you need a library that's not built into Colab's environment, you can add them as follows:"],"metadata":{"id":"AO22Rtci52aU"}},{"cell_type":"code","source":["## installing relevant packages\n","# !pip install google-search-results package2 package3 ...\n","!pip install https://github.com/pandas-profiling/pandas-profiling/archive/master.zip\n","!pip install markupsafe~=2.1.1\n","!pip install folium==0.2.1\n","!pip install markupsafe==2.0.1\n","!pip install imbalanced-learn\n","!pip install scikit-learn-extra\n","!pip install factor_analyzer\n","!pip install --upgrade category_encoders"],"metadata":{"id":"HqTeM9Fs5zTV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652548167209,"user_tz":240,"elapsed":57940,"user":{"displayName":"Idris Hanafi","userId":"08993226175925256732"}},"outputId":"415b89c1-8903-4a98-a103-9701eed9c39a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting https://github.com/pandas-profiling/pandas-profiling/archive/master.zip\n","  Downloading https://github.com/pandas-profiling/pandas-profiling/archive/master.zip\n","\u001b[K     \\ 21.8 MB 148 kB/s\n","\u001b[?25hRequirement already satisfied: joblib~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling==3.2.0) (1.1.0)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling==3.2.0) (1.4.1)\n","Requirement already satisfied: pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,>=0.25.3 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling==3.2.0) (1.3.5)\n","Requirement already satisfied: matplotlib>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling==3.2.0) (3.2.2)\n","Collecting pydantic>=1.8.1\n","  Downloading pydantic-1.9.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB)\n","\u001b[K     |████████████████████████████████| 10.9 MB 8.6 MB/s \n","\u001b[?25hCollecting PyYAML>=5.0.0\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 52.0 MB/s \n","\u001b[?25hRequirement already satisfied: jinja2>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling==3.2.0) (2.11.3)\n","Collecting visions[type_image_path]==0.7.5\n","  Downloading visions-0.7.5-py3-none-any.whl (102 kB)\n","\u001b[K     |████████████████████████████████| 102 kB 39.4 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling==3.2.0) (1.21.6)\n","Collecting htmlmin>=0.1.12\n","  Downloading htmlmin-0.1.12.tar.gz (19 kB)\n","Requirement already satisfied: missingno>=0.4.2 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling==3.2.0) (0.5.1)\n","Collecting phik>=0.11.1\n","  Downloading phik-0.12.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (690 kB)\n","\u001b[K     |████████████████████████████████| 690 kB 44.6 MB/s \n","\u001b[?25hCollecting tangled-up-in-unicode==0.2.0\n","  Downloading tangled_up_in_unicode-0.2.0-py3-none-any.whl (4.7 MB)\n","\u001b[K     |████████████████████████████████| 4.7 MB 39.9 MB/s \n","\u001b[?25hCollecting requests>=2.24.0\n","  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n","\u001b[K     |████████████████████████████████| 63 kB 1.6 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling==3.2.0) (4.64.0)\n","Requirement already satisfied: seaborn>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling==3.2.0) (0.11.2)\n","Collecting multimethod>=1.4\n","  Downloading multimethod-1.8-py3-none-any.whl (9.8 kB)\n","Requirement already satisfied: attrs>=19.3.0 in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.5->pandas-profiling==3.2.0) (21.4.0)\n","Requirement already satisfied: networkx>=2.4 in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.5->pandas-profiling==3.2.0) (2.6.3)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.5->pandas-profiling==3.2.0) (7.1.2)\n","Collecting imagehash\n","  Downloading ImageHash-4.2.1.tar.gz (812 kB)\n","\u001b[K     |████████████████████████████████| 812 kB 62.9 MB/s \n","\u001b[?25hRequirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2>=2.11.1->pandas-profiling==3.2.0) (2.0.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.0->pandas-profiling==3.2.0) (1.4.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.0->pandas-profiling==3.2.0) (0.11.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.0->pandas-profiling==3.2.0) (3.0.8)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.0->pandas-profiling==3.2.0) (2.8.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=3.2.0->pandas-profiling==3.2.0) (4.2.0)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,>=0.25.3->pandas-profiling==3.2.0) (2022.1)\n","Collecting scipy>=1.4.1\n","  Downloading scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.1 MB)\n","\u001b[K     |████████████████████████████████| 38.1 MB 1.3 MB/s \n","\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=3.2.0->pandas-profiling==3.2.0) (1.15.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->pandas-profiling==3.2.0) (2.10)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->pandas-profiling==3.2.0) (1.24.3)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->pandas-profiling==3.2.0) (2.0.12)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->pandas-profiling==3.2.0) (2021.10.8)\n","Requirement already satisfied: PyWavelets in /usr/local/lib/python3.7/dist-packages (from imagehash->visions[type_image_path]==0.7.5->pandas-profiling==3.2.0) (1.3.0)\n","Building wheels for collected packages: pandas-profiling, htmlmin, imagehash\n","  Building wheel for pandas-profiling (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pandas-profiling: filename=pandas_profiling-3.2.0-py2.py3-none-any.whl size=262599 sha256=085241c35cdd6a04a61ddcf8b992cf35e9157b84ce1612bace67cd3a45eefe8a\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-sh74457k/wheels/cc/d5/09/083fb07c9363a2f45854b0e3a7de7d7c560f07da74b9e9769d\n","  Building wheel for htmlmin (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for htmlmin: filename=htmlmin-0.1.12-py3-none-any.whl size=27098 sha256=2e6f25ea7075d9fac762b3ce73d0442cc36a2dc52c1282e96b5782c86fd977fe\n","  Stored in directory: /root/.cache/pip/wheels/70/e1/52/5b14d250ba868768823940c3229e9950d201a26d0bd3ee8655\n","  Building wheel for imagehash (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for imagehash: filename=ImageHash-4.2.1-py2.py3-none-any.whl size=295206 sha256=151a571406957cdab8fec71ca71f57ec3c1c738eecb222d3974e4d6dc80b13fc\n","  Stored in directory: /root/.cache/pip/wheels/4c/d5/59/5e3e297533ddb09407769762985d134135064c6831e29a914e\n","Successfully built pandas-profiling htmlmin imagehash\n","Installing collected packages: tangled-up-in-unicode, scipy, multimethod, visions, imagehash, requests, PyYAML, pydantic, phik, htmlmin, pandas-profiling\n","  Attempting uninstall: scipy\n","    Found existing installation: scipy 1.4.1\n","    Uninstalling scipy-1.4.1:\n","      Successfully uninstalled scipy-1.4.1\n","  Attempting uninstall: requests\n","    Found existing installation: requests 2.23.0\n","    Uninstalling requests-2.23.0:\n","      Successfully uninstalled requests-2.23.0\n","  Attempting uninstall: PyYAML\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","  Attempting uninstall: pandas-profiling\n","    Found existing installation: pandas-profiling 1.4.1\n","    Uninstalling pandas-profiling-1.4.1:\n","      Successfully uninstalled pandas-profiling-1.4.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n","albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Successfully installed PyYAML-6.0 htmlmin-0.1.12 imagehash-4.2.1 multimethod-1.8 pandas-profiling-3.2.0 phik-0.12.2 pydantic-1.9.0 requests-2.27.1 scipy-1.7.3 tangled-up-in-unicode-0.2.0 visions-0.7.5\n","Collecting markupsafe~=2.1.1\n","  Downloading MarkupSafe-2.1.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n","Installing collected packages: markupsafe\n","  Attempting uninstall: markupsafe\n","    Found existing installation: MarkupSafe 2.0.1\n","    Uninstalling MarkupSafe-2.0.1:\n","      Successfully uninstalled MarkupSafe-2.0.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n","Successfully installed markupsafe-2.1.1\n","Collecting folium==0.2.1\n","  Downloading folium-0.2.1.tar.gz (69 kB)\n","\u001b[K     |████████████████████████████████| 69 kB 3.7 MB/s \n","\u001b[?25hRequirement already satisfied: Jinja2 in /usr/local/lib/python3.7/dist-packages (from folium==0.2.1) (2.11.3)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2->folium==0.2.1) (2.1.1)\n","Building wheels for collected packages: folium\n","  Building wheel for folium (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for folium: filename=folium-0.2.1-py3-none-any.whl size=79808 sha256=40905ab50f1083262e82cbc5bb66fc67d4534a723a981c1157cd9df6db58a567\n","  Stored in directory: /root/.cache/pip/wheels/9a/f0/3a/3f79a6914ff5affaf50cabad60c9f4d565283283c97f0bdccf\n","Successfully built folium\n","Installing collected packages: folium\n","  Attempting uninstall: folium\n","    Found existing installation: folium 0.8.3\n","    Uninstalling folium-0.8.3:\n","      Successfully uninstalled folium-0.8.3\n","Successfully installed folium-0.2.1\n","Collecting markupsafe==2.0.1\n","  Downloading MarkupSafe-2.0.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (31 kB)\n","Installing collected packages: markupsafe\n","  Attempting uninstall: markupsafe\n","    Found existing installation: MarkupSafe 2.1.1\n","    Uninstalling MarkupSafe-2.1.1:\n","      Successfully uninstalled MarkupSafe-2.1.1\n","Successfully installed markupsafe-2.0.1\n","Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.7/dist-packages (0.8.1)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.21.6)\n","Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.7.3)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.1.0)\n","Requirement already satisfied: scikit-learn>=0.24 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.0.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24->imbalanced-learn) (3.1.0)\n","Collecting scikit-learn-extra\n","  Downloading scikit_learn_extra-0.2.0-cp37-cp37m-manylinux2010_x86_64.whl (1.7 MB)\n","\u001b[K     |████████████████████████████████| 1.7 MB 8.0 MB/s \n","\u001b[?25hRequirement already satisfied: scikit-learn>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn-extra) (1.0.2)\n","Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn-extra) (1.7.3)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn-extra) (1.21.6)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.23.0->scikit-learn-extra) (3.1.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.23.0->scikit-learn-extra) (1.1.0)\n","Installing collected packages: scikit-learn-extra\n","Successfully installed scikit-learn-extra-0.2.0\n","Collecting factor_analyzer\n","  Downloading factor_analyzer-0.4.0.tar.gz (41 kB)\n","\u001b[K     |████████████████████████████████| 41 kB 425 kB/s \n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from factor_analyzer) (1.3.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from factor_analyzer) (1.7.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from factor_analyzer) (1.21.6)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from factor_analyzer) (1.0.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->factor_analyzer) (2022.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->factor_analyzer) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->factor_analyzer) (1.15.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->factor_analyzer) (1.1.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->factor_analyzer) (3.1.0)\n","Building wheels for collected packages: factor-analyzer\n","  Building wheel for factor-analyzer (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for factor-analyzer: filename=factor_analyzer-0.4.0-py3-none-any.whl size=41455 sha256=021c2c30eea211a00d3bdbc07e9e4b61db234f0ceb212a6869b060ba83579ed6\n","  Stored in directory: /root/.cache/pip/wheels/ac/00/37/1f0e8a5039f9e9f207c4405bbce0796f07701eb377bfc6cc76\n","Successfully built factor-analyzer\n","Installing collected packages: factor-analyzer\n","Successfully installed factor-analyzer-0.4.0\n","Requirement already satisfied: factor_analyzer in /usr/local/lib/python3.7/dist-packages (0.4.0)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from factor_analyzer) (1.0.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from factor_analyzer) (1.7.3)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from factor_analyzer) (1.3.5)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from factor_analyzer) (1.21.6)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->factor_analyzer) (2022.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->factor_analyzer) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->factor_analyzer) (1.15.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->factor_analyzer) (3.1.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->factor_analyzer) (1.1.0)\n","Collecting category_encoders\n","  Downloading category_encoders-2.4.1-py2.py3-none-any.whl (80 kB)\n","\u001b[K     |████████████████████████████████| 80 kB 4.7 MB/s \n","\u001b[?25hRequirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (1.0.2)\n","Requirement already satisfied: pandas>=0.21.1 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (1.3.5)\n","Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (1.7.3)\n","Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (0.10.2)\n","Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (0.5.2)\n","Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (1.21.6)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.21.1->category_encoders) (2022.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.21.1->category_encoders) (2.8.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from patsy>=0.5.1->category_encoders) (1.15.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->category_encoders) (3.1.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->category_encoders) (1.1.0)\n","Installing collected packages: category-encoders\n","Successfully installed category-encoders-2.4.1\n"]}]},{"cell_type":"markdown","source":["### Step 1: Data Extraction Portion\n","\n","Goal for this step: \n","- Pull from BigQuery and generate 6 months frozen dataset\n","\n","TODO: \n","- [x] @Tia to refactor the extraction to reflect the notebook: `Data_Processing.ipynb`\n","- [Optional] @Tia refine dates logic to be customizable\n","- [x] @Tia add argparse\n"],"metadata":{"id":"ypwxLHiYw1g8"}},{"cell_type":"code","source":["!python data_extraction.py \\\n","  --output_directory ../datasets/monthly-partitioned-data/ \\\n","  --credentials_json ../credentials/compact-scene-317315-56e479e9e148.json"],"metadata":{"id":"yss61KQWwwIZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Step 2: Transaction Extraction Portion\n","\n","**WARNING: THIS TAKES ~12 HOURS TO RUN**\n","\n","Goal:\n","- Pull all Transactions from monthly datasets from Step 1\n","- Pull all Non-Transactions from monthly datasets from Step 1\n","\n","TODO: \n","- [x] @Idris to refactor the extraction to reflect the notebook and pull all transactions from each monthly dataset: `Extraction_Data_Sample.ipynb`\n","- [x] @Idris add argparse\n"],"metadata":{"id":"f1UoZwT9xZ_K"}},{"cell_type":"code","source":["%%time\n","!python transaction_extraction.py \\\n","  --input_directory ../datasets/monthly_partitioned_data/ \\\n","  --output_directory ../datasets/monthly_partitioned_data_transactions/"],"metadata":{"id":"6Y-vxJUrn12f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Step 3: Data Sampling Portion\n","\n","Goal:\n","1. Reads all transactions CSV from the input directory and join as one DF.\n","2. Reads all non-transactions CSV from the input directory and samples 10%\n","3. Join both results from Step 1 and 2 into one DF called `sample_dataset.csv`\n","\n","TODO: \n","- [x] @Idris to refactor the extraction to reflect the notebook: `Data_Processing.ipynb`\n","- [x] @Idris add argparse\n"],"metadata":{"id":"d5d9FHOn0MSu"}},{"cell_type":"code","source":["%%time\n","!python create_sample_dataset.py \\\n","  --input_directory ../datasets/monthly_partitioned_data_transactions/ \\\n","  --output_file ../datasets/sample_dataset.csv"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UxRpydk80LR2","executionInfo":{"status":"ok","timestamp":1652315448363,"user_tz":240,"elapsed":363178,"user":{"displayName":"Idris Hanafi","userId":"08993226175925256732"}},"outputId":"bc3f2555-5b10-4625-d63e-b41bb0debb60"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["../datasets/monthly_partitioned_data_transactions/ ../datasets/sample_dataset.csv\n","['../datasets/monthly_partitioned_data_transactions/non_transactions/non_transactions_from_January 2017 Google Analytics Dataset.csv', '../datasets/monthly_partitioned_data_transactions/non_transactions/non_transactions_from_August 2016 Google Analytics Dataset.csv', '../datasets/monthly_partitioned_data_transactions/non_transactions/non_transactions_from_September 2016 Google Analytics Dataset.csv', '../datasets/monthly_partitioned_data_transactions/non_transactions/non_transactions_from_October 2016 Google Analytics Dataset.csv', '../datasets/monthly_partitioned_data_transactions/non_transactions/non_transactions_from_November 2016 Google Analytics Dataset.csv', '../datasets/monthly_partitioned_data_transactions/non_transactions/non_transactions_from_December 2016 Google Analytics Dataset.csv']\n","100% 6/6 [03:32<00:00, 35.37s/it]\n","CPU times: user 2.91 s, sys: 595 ms, total: 3.5 s\n","Wall time: 6min 2s\n"]}]},{"cell_type":"markdown","source":["### Step 4: Common Data Cleaning & Descriptive Analysis Portion\n","\n","Goal:\n","- Drop unneeded columns (Refer to this [link](https://docs.google.com/spreadsheets/d/1fT-iZyGZnpkli9ve9EY9TRTfycNlIoxp/edit?usp=sharing&ouid=116636835356831800242&rtpof=true&sd=true))\n","- Cast to appropriate datatypes\n","\n","On a different notebook:\n","- Visualizations\n","- EDA\n","- Describing Data\n","- Histograms\n","- Dashboarding ([Pandas Profiling](https://github.com/ydataai/pandas-profiling))\n","\n","Assigned: \n","- [x] @Tia\n"],"metadata":{"id":"UA0knB0k1Dov"}},{"cell_type":"code","source":["!python clean_dataset.py \\\n","  --input_file ../datasets/sample_dataset.csv \\\n","  --output_file ../datasets/cleaned_dataset.csv \\\n","  --dashboard_file ../datasets/data_dashboard.html"],"metadata":{"id":"fRLHvfl22Cx-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652408148446,"user_tz":420,"elapsed":88592,"user":{"displayName":"Tia Burton","userId":"09352393735441630552"}},"outputId":"65bc99ef-a4f1-4ae4-9dab-5797d3d19c8d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tcmalloc: large alloc 1073741824 bytes == 0x4654c000 @  0x7f76673da2a4 0x7f765582e9a5 0x7f765582fcc1 0x7f765583169e 0x7f765580250c 0x7f765580f399 0x7f76557f797a 0x59afff 0x515655 0x549e0e 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x593dd7 0x5118f8 0x549576 0x4bcb19 0x5134a6 0x549e0e 0x593fce 0x548ae9 0x51566f 0x549576 0x604173 0x5f5506 0x5f8c6c 0x5f9206 0x64faf2 0x64fc4e\n","tcmalloc: large alloc 2147483648 bytes == 0x96d5e000 @  0x7f76673da2a4 0x7f765582e9a5 0x7f765582fcc1 0x7f765583169e 0x7f765580250c 0x7f765580f399 0x7f76557f797a 0x59afff 0x515655 0x549e0e 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x593dd7 0x5118f8 0x549576 0x4bcb19 0x5134a6 0x549e0e 0x593fce 0x548ae9 0x51566f 0x549576 0x604173 0x5f5506 0x5f8c6c 0x5f9206 0x64faf2 0x64fc4e\n","Summarize dataset: 100% 101/101 [00:30<00:00,  3.31it/s, Completed]\n","Generate report structure: 100% 1/1 [00:06<00:00,  6.35s/it]\n","Render HTML: 100% 1/1 [00:04<00:00,  4.24s/it]\n","Export report to file: 100% 1/1 [00:00<00:00,  1.28it/s]\n"]}]},{"cell_type":"markdown","source":["### Step 5: Data Prep for Modeling\n","\n","Goal:\n","- Create 4 datasets as inputs for 4 models\n","- Prep should include:\n","  * Column Dropping\n","  * Column Encoding\n","\n","The 4 datasets for each model should be:\n","- A1- Likelyhood to convert data set (Visitors) (Assigned: @Fred)\n","- B2- Complex clustering data set (Visitors) (Assigned: @Fred)\n","- B1- RFM data set (Assigned: @Fred)\n","- A2- Returning customers data set (Assigned: @Idris)\n","- [Optional] A3- Attribution model data set (Assigned: @Tia)\n"],"metadata":{"id":"dgI0faFXVf6x"}},{"cell_type":"code","source":["!python data_prep.py \\\n","  --input_file ../datasets/cleaned_dataset.csv \\\n","  --output_directory ../datasets/model_files"],"metadata":{"id":"HHsgR42lWKmM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652548692310,"user_tz":240,"elapsed":7088,"user":{"displayName":"Idris Hanafi","userId":"08993226175925256732"}},"outputId":"23a88137-a477-4436-ae0b-6eaa4e14aa6f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","  import pandas.util.testing as tm\n","../datasets/cleaned_dataset.csv ../datasets/model_files\n","2016-08-01 00:00:00 succesfully transformed to a datime object\n","Divided by 10^6\n","Transformations being made\n","1 date NO CHANGE\n","2 fullVisitorId NO CHANGE\n","3 socialEngagementType fillnan\n","3 socialEngagementType binary\n","3 socialEngagementType int64\n","4 channelGrouping one_hot drop\n","5 totals.hits int64\n","5 totals.hits NO CHANGE\n","6 totals.pageviews fillnan\n","7 totals.timeOnSite fillnan\n","8 totals.transactions fillnan\n","9 totals.newVisits fillnan\n","9 totals.newVisits int64\n","10 hits.eCommerceAction.action_type int64\n","10 hits.eCommerceAction.action_type NO CHANGE\n","11 totals.bounces fillnan\n","11 totals.bounces int64\n","0.1183562197092084\n","12 geoNetwork.country geoNetwork.country_woe woe\n","12 geoNetwork.country drop\n","0.1183562197092084\n","13 trafficSource.source trafficSource.source_woe woe\n","13 trafficSource.source drop\n","14 trafficSource.medium one_hot drop\n","15 trafficSource.isTrueDirect fillnan\n","cat_code trafficSource.isTrueDirect\n","0.1183562197092084\n","16 device.browser device.browser_woe woe\n","16 device.browser drop\n","17 device.operatingSystem one_hot drop\n","18 device.deviceCategory one_hot drop\n","19 hits.type drop\n","20 hits.hour drop\n","21 hits.minute drop\n","22 Monetary fillnan\n","23 Recency NO CHANGE\n","24 Frequency NO CHANGE\n","25 buyers NO CHANGE\n","26 repurchasers int64\n","26 repurchasers NO CHANGE\n","succesfully exporting changes file to csv\n","succesfully exporting both A1_B2 files to csv\n","succesfully exporting A2 to csv\n","succesfully exporting B1 rfm to csv\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","\n","# df = pd.read_csv(\"../datasets/model_files/A1_B2_data.csv\")\n","df = pd.read_csv(\"../datasets/model_files/changes_made.csv\")"],"metadata":{"id":"LcdUG8Hkk_3O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Modeling Pipeline\n","\n","Goal:\n","- @All test out our input dataset against our models and discuss results"],"metadata":{"id":"OIhutrPtVtLF"}}]}