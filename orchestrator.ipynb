{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "orchestrator.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tiaaburton/Milestone-II-Machine-Learning-Project/blob/main/orchestrator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pipeline Workflow\n",
        "\n",
        "This noteboook includes the workflow for both processes:\n",
        "1. Data Pipeline\n",
        "2. Model Pipeline\n",
        "\n",
        "TODO:\n",
        "- @All align on the python files/naming conventions and each steps\n",
        "- @All make sure the steps reflect the one Fred proposed"
      ],
      "metadata": {
        "id": "d3ZkjF-yw6We"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZGmt1OuwdLb",
        "outputId": "7150f346-58cb-4add-9128-04543d79c806"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"/content/drive/Shareddrives/SIADS - 694-695 Team Drive/python-files\"\n",
        "!ls -l"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYasLeTo5BIq",
        "outputId": "17551e96-5b1b-4430-dc0e-f119db7acba6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/Shareddrives/SIADS - 694-695 Team Drive/python-files\n",
            "total 84\n",
            "-rw------- 1 root root  1655 May 13 02:13 clean_dataset.py\n",
            "-rw------- 1 root root  2434 May 12 00:19 create_sample_dataset.py\n",
            "-rw------- 1 root root  3327 May 11 22:09 data_extraction.py\n",
            "-rw------- 1 root root 17254 May 12 20:26 data_prep.py\n",
            "-rw------- 1 root root     0 Apr 23 23:30 data_preprocessing.py\n",
            "-rw------- 1 root root   165 May  3 22:13 ml_algo.py\n",
            "-rw------- 1 root root 38949 May 13 02:13 orchestrator.ipynb\n",
            "drwx------ 2 root root  4096 May 11 04:15 __pycache__\n",
            "-rw------- 1 root root 10931 May 12 17:51 transaction_extraction.py\n",
            "-rw------- 1 root root  4130 May 12 00:15 utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5zc5gw9wnxZ",
        "outputId": "025151c0-bc11-4d09-be0f-3eca41fe282e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.7.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you need a library that's not built into Colab's environment, you can add them as follows:"
      ],
      "metadata": {
        "id": "AO22Rtci52aU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install google-search-results package2 package3 ...\n",
        "!pip install https://github.com/pandas-profiling/pandas-profiling/archive/master.zip\n",
        "!pip install markupsafe~=2.1.1\n",
        "!pip install folium==0.2.1\n",
        "!pip install markupsafe==2.0.1"
      ],
      "metadata": {
        "id": "HqTeM9Fs5zTV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40612809-b0cf-44ca-e9dd-0c2f92256875"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting https://github.com/pandas-profiling/pandas-profiling/archive/master.zip\n",
            "  Downloading https://github.com/pandas-profiling/pandas-profiling/archive/master.zip\n",
            "\u001b[K     - 21.8 MB 152 kB/s\n",
            "\u001b[?25hRequirement already satisfied: joblib~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling==3.2.0) (1.1.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling==3.2.0) (1.4.1)\n",
            "Requirement already satisfied: pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,>=0.25.3 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling==3.2.0) (1.3.5)\n",
            "Requirement already satisfied: matplotlib>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling==3.2.0) (3.2.2)\n",
            "Collecting pydantic>=1.8.1\n",
            "  Downloading pydantic-1.9.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.9 MB 11.9 MB/s \n",
            "\u001b[?25hCollecting PyYAML>=5.0.0\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 63.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jinja2>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling==3.2.0) (2.11.3)\n",
            "Collecting visions[type_image_path]==0.7.5\n",
            "  Downloading visions-0.7.5-py3-none-any.whl (102 kB)\n",
            "\u001b[K     |████████████████████████████████| 102 kB 78.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling==3.2.0) (1.21.6)\n",
            "Collecting htmlmin>=0.1.12\n",
            "  Downloading htmlmin-0.1.12.tar.gz (19 kB)\n",
            "Requirement already satisfied: missingno>=0.4.2 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling==3.2.0) (0.5.1)\n",
            "Collecting phik>=0.11.1\n",
            "  Downloading phik-0.12.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (690 kB)\n",
            "\u001b[K     |████████████████████████████████| 690 kB 62.7 MB/s \n",
            "\u001b[?25hCollecting tangled-up-in-unicode==0.2.0\n",
            "  Downloading tangled_up_in_unicode-0.2.0-py3-none-any.whl (4.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 60.7 MB/s \n",
            "\u001b[?25hCollecting requests>=2.24.0\n",
            "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling==3.2.0) (4.64.0)\n",
            "Requirement already satisfied: seaborn>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling==3.2.0) (0.11.2)\n",
            "Collecting multimethod>=1.4\n",
            "  Downloading multimethod-1.8-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: attrs>=19.3.0 in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.5->pandas-profiling==3.2.0) (21.4.0)\n",
            "Requirement already satisfied: networkx>=2.4 in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.5->pandas-profiling==3.2.0) (2.6.3)\n",
            "Collecting imagehash\n",
            "  Downloading ImageHash-4.2.1.tar.gz (812 kB)\n",
            "\u001b[K     |████████████████████████████████| 812 kB 91.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.5->pandas-profiling==3.2.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2>=2.11.1->pandas-profiling==3.2.0) (2.0.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.0->pandas-profiling==3.2.0) (1.4.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.0->pandas-profiling==3.2.0) (3.0.8)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.0->pandas-profiling==3.2.0) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.0->pandas-profiling==3.2.0) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=3.2.0->pandas-profiling==3.2.0) (4.2.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,>=0.25.3->pandas-profiling==3.2.0) (2022.1)\n",
            "Collecting scipy>=1.4.1\n",
            "  Downloading scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 38.1 MB 1.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=3.2.0->pandas-profiling==3.2.0) (1.15.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->pandas-profiling==3.2.0) (1.24.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->pandas-profiling==3.2.0) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->pandas-profiling==3.2.0) (2021.10.8)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->pandas-profiling==3.2.0) (2.10)\n",
            "Requirement already satisfied: PyWavelets in /usr/local/lib/python3.7/dist-packages (from imagehash->visions[type_image_path]==0.7.5->pandas-profiling==3.2.0) (1.3.0)\n",
            "Building wheels for collected packages: pandas-profiling, htmlmin, imagehash\n",
            "  Building wheel for pandas-profiling (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pandas-profiling: filename=pandas_profiling-3.2.0-py2.py3-none-any.whl size=262599 sha256=687458324f8ce7c6f3f6993a72c50256c9dad423ec2e166396536e5b83a70d3b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-v1pfj2qi/wheels/cc/d5/09/083fb07c9363a2f45854b0e3a7de7d7c560f07da74b9e9769d\n",
            "  Building wheel for htmlmin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for htmlmin: filename=htmlmin-0.1.12-py3-none-any.whl size=27098 sha256=6008e2a6e6d2c1264fff94b34ddc7c62fa55160cb1ae323a674431f9175279d4\n",
            "  Stored in directory: /root/.cache/pip/wheels/70/e1/52/5b14d250ba868768823940c3229e9950d201a26d0bd3ee8655\n",
            "  Building wheel for imagehash (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for imagehash: filename=ImageHash-4.2.1-py2.py3-none-any.whl size=295206 sha256=25eca6479b3f972503005a5921e12fddabe6a71086115a64238f6043a7b812f0\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/d5/59/5e3e297533ddb09407769762985d134135064c6831e29a914e\n",
            "Successfully built pandas-profiling htmlmin imagehash\n",
            "Installing collected packages: tangled-up-in-unicode, scipy, multimethod, visions, imagehash, requests, PyYAML, pydantic, phik, htmlmin, pandas-profiling\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: pandas-profiling\n",
            "    Found existing installation: pandas-profiling 1.4.1\n",
            "    Uninstalling pandas-profiling-1.4.1:\n",
            "      Successfully uninstalled pandas-profiling-1.4.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed PyYAML-6.0 htmlmin-0.1.12 imagehash-4.2.1 multimethod-1.8 pandas-profiling-3.2.0 phik-0.12.2 pydantic-1.9.0 requests-2.27.1 scipy-1.7.3 tangled-up-in-unicode-0.2.0 visions-0.7.5\n",
            "Collecting markupsafe~=2.1.1\n",
            "  Downloading MarkupSafe-2.1.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Installing collected packages: markupsafe\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 2.0.1\n",
            "    Uninstalling MarkupSafe-2.0.1:\n",
            "      Successfully uninstalled MarkupSafe-2.0.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed markupsafe-2.1.1\n",
            "Collecting folium==0.2.1\n",
            "  Downloading folium-0.2.1.tar.gz (69 kB)\n",
            "\u001b[K     |████████████████████████████████| 69 kB 417 kB/s \n",
            "\u001b[?25hRequirement already satisfied: Jinja2 in /usr/local/lib/python3.7/dist-packages (from folium==0.2.1) (2.11.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2->folium==0.2.1) (2.1.1)\n",
            "Building wheels for collected packages: folium\n",
            "  Building wheel for folium (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for folium: filename=folium-0.2.1-py3-none-any.whl size=79808 sha256=d5b854ed51372415a401b35faee6213fd827301c029cab35db32a53012c13a0c\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/f0/3a/3f79a6914ff5affaf50cabad60c9f4d565283283c97f0bdccf\n",
            "Successfully built folium\n",
            "Installing collected packages: folium\n",
            "  Attempting uninstall: folium\n",
            "    Found existing installation: folium 0.8.3\n",
            "    Uninstalling folium-0.8.3:\n",
            "      Successfully uninstalled folium-0.8.3\n",
            "Successfully installed folium-0.2.1\n",
            "Collecting markupsafe==2.0.1\n",
            "  Downloading MarkupSafe-2.0.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (31 kB)\n",
            "Installing collected packages: markupsafe\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 2.1.1\n",
            "    Uninstalling MarkupSafe-2.1.1:\n",
            "      Successfully uninstalled MarkupSafe-2.1.1\n",
            "Successfully installed markupsafe-2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Data Extraction Portion\n",
        "\n",
        "Goal for this step: \n",
        "- Pull from BigQuery and generate 6 months frozen dataset\n",
        "\n",
        "TODO: \n",
        "- [x] @Tia to refactor the extraction to reflect the notebook: `Data_Processing.ipynb`\n",
        "- [Optional] @Tia refine dates logic to be customizable\n",
        "- [x] @Tia add argparse\n"
      ],
      "metadata": {
        "id": "ypwxLHiYw1g8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python data_extraction.py \\\n",
        "  --output_directory datasets/monthly-partitioned-datasets/"
      ],
      "metadata": {
        "id": "yss61KQWwwIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Transaction Extraction Portion\n",
        "\n",
        "**WARNING: THIS TAKES ~12 HOURS TO RUN**\n",
        "\n",
        "Goal:\n",
        "- Pull all Transactions from monthly datasets from Step 1\n",
        "- Pull all Non-Transactions from monthly datasets from Step 1\n",
        "\n",
        "TODO: \n",
        "- [x] @Idris to refactor the extraction to reflect the notebook and pull all transactions from each monthly dataset: `Extraction_Data_Sample.ipynb`\n",
        "- [x] @Idris add argparse\n"
      ],
      "metadata": {
        "id": "f1UoZwT9xZ_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "!python transaction_extraction.py \\\n",
        "  --input_directory ../datasets/monthly_partitioned_data/ \\\n",
        "  --output_directory ../datasets/monthly_partitioned_data_transactions/"
      ],
      "metadata": {
        "id": "6Y-vxJUrn12f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Data Sampling Portion\n",
        "\n",
        "Goal:\n",
        "1. Reads all transactions CSV from the input directory and join as one DF.\n",
        "2. Reads all non-transactions CSV from the input directory and samples 10%\n",
        "3. Join both results from Step 1 and 2 into one DF called `sample_dataset.csv`\n",
        "\n",
        "TODO: \n",
        "- [x] @Idris to refactor the extraction to reflect the notebook: `Data_Processing.ipynb`\n",
        "- [x] @Idris add argparse\n"
      ],
      "metadata": {
        "id": "d5d9FHOn0MSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "!python create_sample_dataset.py \\\n",
        "  --input_directory ../datasets/monthly_partitioned_data_transactions/ \\\n",
        "  --output_file ../datasets/sample_dataset.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxRpydk80LR2",
        "outputId": "bc3f2555-5b10-4625-d63e-b41bb0debb60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "../datasets/monthly_partitioned_data_transactions/ ../datasets/sample_dataset.csv\n",
            "['../datasets/monthly_partitioned_data_transactions/non_transactions/non_transactions_from_January 2017 Google Analytics Dataset.csv', '../datasets/monthly_partitioned_data_transactions/non_transactions/non_transactions_from_August 2016 Google Analytics Dataset.csv', '../datasets/monthly_partitioned_data_transactions/non_transactions/non_transactions_from_September 2016 Google Analytics Dataset.csv', '../datasets/monthly_partitioned_data_transactions/non_transactions/non_transactions_from_October 2016 Google Analytics Dataset.csv', '../datasets/monthly_partitioned_data_transactions/non_transactions/non_transactions_from_November 2016 Google Analytics Dataset.csv', '../datasets/monthly_partitioned_data_transactions/non_transactions/non_transactions_from_December 2016 Google Analytics Dataset.csv']\n",
            "100% 6/6 [03:32<00:00, 35.37s/it]\n",
            "CPU times: user 2.91 s, sys: 595 ms, total: 3.5 s\n",
            "Wall time: 6min 2s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Common Data Cleaning & Descriptive Analysis Portion\n",
        "\n",
        "Goal:\n",
        "- Unnest columns with final columns file\n",
        "- Drop unneeded columns (Refer to this [link](https://docs.google.com/spreadsheets/d/1fT-iZyGZnpkli9ve9EY9TRTfycNlIoxp/edit?usp=sharing&ouid=116636835356831800242&rtpof=true&sd=true))\n",
        "- Cast to appropriate datatypes\n",
        "\n",
        "On a different notebook:\n",
        "- Visualizations\n",
        "- EDA\n",
        "- Describing Data\n",
        "- Histograms\n",
        "- Dashboarding ([Pandas Profiling](https://github.com/ydataai/pandas-profiling))\n",
        "\n",
        "Assigned: @Tia\n"
      ],
      "metadata": {
        "id": "UA0knB0k1Dov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python clean_dataset.py \\\n",
        "  --input_file ../datasets/sample_dataset.csv \\\n",
        "  --output_file ../datasets/cleaned_dataset.csv \\\n",
        "  --dashboard_file ../datasets/data_dashboard.html"
      ],
      "metadata": {
        "id": "fRLHvfl22Cx-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65bc99ef-a4f1-4ae4-9dab-5797d3d19c8d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tcmalloc: large alloc 1073741824 bytes == 0x4654c000 @  0x7f76673da2a4 0x7f765582e9a5 0x7f765582fcc1 0x7f765583169e 0x7f765580250c 0x7f765580f399 0x7f76557f797a 0x59afff 0x515655 0x549e0e 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x593dd7 0x5118f8 0x549576 0x4bcb19 0x5134a6 0x549e0e 0x593fce 0x548ae9 0x51566f 0x549576 0x604173 0x5f5506 0x5f8c6c 0x5f9206 0x64faf2 0x64fc4e\n",
            "tcmalloc: large alloc 2147483648 bytes == 0x96d5e000 @  0x7f76673da2a4 0x7f765582e9a5 0x7f765582fcc1 0x7f765583169e 0x7f765580250c 0x7f765580f399 0x7f76557f797a 0x59afff 0x515655 0x549e0e 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x593dd7 0x5118f8 0x549576 0x4bcb19 0x5134a6 0x549e0e 0x593fce 0x548ae9 0x51566f 0x549576 0x604173 0x5f5506 0x5f8c6c 0x5f9206 0x64faf2 0x64fc4e\n",
            "Summarize dataset: 100% 101/101 [00:30<00:00,  3.31it/s, Completed]\n",
            "Generate report structure: 100% 1/1 [00:06<00:00,  6.35s/it]\n",
            "Render HTML: 100% 1/1 [00:04<00:00,  4.24s/it]\n",
            "Export report to file: 100% 1/1 [00:00<00:00,  1.28it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"../datasets/cleaned_dataset.csv\")"
      ],
      "metadata": {
        "id": "45RuEDdb6b2J"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Data Prep for Modeling\n",
        "\n",
        "Goal:\n",
        "- Create 4 datasets as inputs for 4 models\n",
        "- Prep should include:\n",
        "  * Column Dropping\n",
        "  * Column Encoding\n",
        "\n",
        "The 4 datasets for each model should be:\n",
        "- A1- Likelyhood to convert data set (Visitors) (Assigned: @Fred)\n",
        "- B2- Complex clustering data set (Visitors) (Assigned: @Fred)\n",
        "- B1- RFM data set (Assigned: @Fred)\n",
        "- A2- Returning customers data set (Assigned: @Idris)\n",
        "- [Optional] A3- Attribution model data set (Assigned: @Tia)\n"
      ],
      "metadata": {
        "id": "dgI0faFXVf6x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python data_prep.py \\\n",
        "  --input_file ../datasets/cleaned_dataset.csv \\\n",
        "  --output_directory ../datasets/model_files/"
      ],
      "metadata": {
        "id": "HHsgR42lWKmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling Pipeline\n",
        "\n",
        "Goal:\n",
        "- @All test out our input dataset against our models and discuss results"
      ],
      "metadata": {
        "id": "OIhutrPtVtLF"
      }
    }
  ]
}